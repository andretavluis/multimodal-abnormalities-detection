{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/philferriere/cocoapi.git to c:\\users\\utilizador\\appdata\\local\\temp\\pip-req-build-yh1gskmk\n",
      "  Resolved https://github.com/philferriere/cocoapi.git to commit 2929bd2ef6b451054755dfd7ceb09278f935f7ad\n",
      "Building wheels for collected packages: pycocotools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/philferriere/cocoapi.git 'C:\\Users\\Utilizador\\AppData\\Local\\Temp\\pip-req-build-yh1gskmk'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp39-cp39-win_amd64.whl size=81991 sha256=2bb632cc2c2b576b4beaca5d6de5a363083304508965cc6df30b271d278e15de\n",
      "  Stored in directory: C:\\Users\\Utilizador\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-zv44n6yy\\wheels\\8d\\25\\59\\5840f59a3c30bac7223e8d0f1dcf19ec4d3c249d7659d9782f\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils.print as print_f\n",
    "\n",
    "from utils.coco_eval import get_eval_params_dict\n",
    "from utils.engine import xami_train_one_epoch, xami_evaluate, get_iou_types\n",
    "from utils.plot import plot_losses, plot_ap_ars\n",
    "from utils.save import get_data_from_metric_logger\n",
    "from utils.coco_utils import get_cocos\n",
    "\n",
    "from models.setup import ModelSetup\n",
    "from models.build import create_multimodal_rcnn_model\n",
    "from models.train import TrainingInfo\n",
    "from utils.save import check_best, end_train\n",
    "from data.load import get_datasets, get_dataloaders\n",
    "from IPython.display import clear_output\n",
    "from utils.eval import get_ap_ar, get_ap_ar_for_train_val\n",
    "from utils.train import get_optimiser, get_lr_scheduler, print_params_setup\n",
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "from models.dynamic_loss import DynamicWeightedLoss\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS, XAMI_MIMIC_PATH\n",
    "from  datetime import datetime\n",
    "import torch.optim as optim\n",
    "\n",
    "## Suppress the assignement warning from pandas.r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "## Supress user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CPU]\n"
     ]
    }
   ],
   "source": [
    "device = clean_memory_get_device()\n",
    "reproducibility()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_iobb = True\n",
    "io_type_str = \"IoBB\" if use_iobb else \"IoU\"\n",
    "labels_cols = DEFAULT_REFLACX_LABEL_COLS\n",
    "iou_thrs = np.array([0.5])\n",
    "\n",
    "common_args = {\n",
    "    \"save_early_stop_model\": True,\n",
    "    \"optimiser\": \"sgd\",\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"image_backbone_pretrained\": True,\n",
    "    \"fixation_backbone_pretrained\": True,\n",
    "    \"record_training_performance\": True,\n",
    "    \"dataset_mode\": \"normal\",\n",
    "    \"image_size\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"warmup_epochs\": 0,\n",
    "    \"lr_scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"reduceLROnPlateau_factor\": 0.1,\n",
    "    \"reduceLROnPlateau_patience\": 999,\n",
    "    \"reduceLROnPlateau_full_stop\": True,\n",
    "    \"multiStepLR_milestones\": 100,\n",
    "    \"multiStepLR_gamma\": 0.1,\n",
    "    \"use_mask\": True,\n",
    "    \"gt_in_train_till\": 999,\n",
    "    \"box_head_dropout_rate\": 0,\n",
    "    \"measure_test\": True,\n",
    "}\n",
    "\n",
    "fusion_add_args = {\n",
    "    \"fuse_depth\": 0,\n",
    "    \"fusion_residule\": False,\n",
    "    \"fusion_strategy\": \"add\", \n",
    "}\n",
    "\n",
    "small_model_args = {\n",
    "    \"mask_hidden_layers\": 64,\n",
    "    \"fuse_conv_channels\": 64,\n",
    "    \"representation_size\": 64, \n",
    "    \"backbone_out_channels\": 64,\n",
    "}\n",
    "\n",
    "mobilenet_args = {\n",
    "    \"backbone\": \"mobilenet_v3\",\n",
    "    \"using_fpn\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Preparing for the training.====================\n",
      "Using pretrained backbone. mobilenet_v3\n",
      "Using pretrained backbone. mobilenet_v3\n",
      "forward_testing_model will use mask, [64] layers.\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "[model]: 4,749,149\n",
      "[model.backbone]: 1,258,848\n",
      "[model.rpn]: 41,803\n",
      "[model.roi_heads]: 2,189,650\n",
      "[model.roi_heads.box_head]: 204,928\n",
      "[model.roi_heads.box_head.fc6]: 200,768\n",
      "[model.roi_heads.box_head.fc7]: 4,160\n",
      "[model.roi_heads.box_predictor]: 975\n",
      "[model.roi_heads.mask_head]: 1,917,952\n",
      "Using SGD as optimizer with lr=0.001\n",
      "====================Start training. Preparing Tooke [28] sec====================\n",
      "====================Training model: [forward_testing_model]====================\n",
      "========================================For Training [forward_testing_model]========================================\n",
      "ModelSetup(dataset_mode='normal', name='forward_testing_model', use_fixations=True, save_early_stop_model=True, record_training_performance=True, backbone='mobilenet_v3', optimiser='sgd', lr=0.001, weight_decay=1e-05, image_backbone_pretrained=True, fixation_backbone_pretrained=True, image_size=512, backbone_out_channels=64, batch_size=4, warmup_epochs=0, lr_scheduler='ReduceLROnPlateau', reduceLROnPlateau_factor=0.1, reduceLROnPlateau_patience=999, reduceLROnPlateau_full_stop=True, multiStepLR_milestones=100, multiStepLR_gamma=0.1, representation_size=64, mask_hidden_layers=64, using_fpn=False, use_mask=True, fuse_conv_channels=64, box_head_dropout_rate=0, fuse_depth=0, fusion_strategy='add', fusion_residule=False, gt_in_train_till=999, measure_test=True)\n",
      "====================================================================================================================\n",
      "\n",
      "Best AP validation model has been saved to: [None]\n",
      "Best AR validation model has been saved to: [None]\n",
      "The final model has been saved to: [None]\n",
      "\n",
      "====================================================================================================================\n",
      "Epoch: [1]  [ 0/11]  eta: 0:00:57  lr: 0.001000  loss: 2.6994 (2.6994)  loss_classifier: 1.0605 (1.0605)  loss_box_reg: 0.0117 (0.0117)  loss_mask: 0.9307 (0.9307)  loss_objectness: 0.6931 (0.6931)  loss_rpn_box_reg: 0.0033 (0.0033)  time: 5.2343  data: 1.4712\n",
      "Epoch: [1]  [10/11]  eta: 0:00:05  lr: 0.001000  loss: 2.0704 (2.1895)  loss_classifier: 0.9667 (0.9420)  loss_box_reg: 0.0091 (0.0101)  loss_mask: 0.4887 (0.5405)  loss_objectness: 0.6928 (0.6928)  loss_rpn_box_reg: 0.0034 (0.0042)  time: 5.2381  data: 1.4313\n",
      "Epoch: [1] Total time: 0:00:57 (5.2387 s / it)\n",
      "Averaged stats: lr: 0.001000  loss: 2.0704 (2.1895)  loss_classifier: 0.9667 (0.9420)  loss_box_reg: 0.0091 (0.0101)  loss_mask: 0.4887 (0.5405)  loss_objectness: 0.6928 (0.6928)  loss_rpn_box_reg: 0.0034 (0.0042)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable MetricLogger object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\UTILIZ~1\\AppData\\Local\\Temp/ipykernel_6276/3156999273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         train_info.last_train_evaluator, train_loger = xami_train_one_epoch(\n\u001b[0m\u001b[0;32m    108\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable MetricLogger object"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_infos = []\n",
    "\n",
    "all_model_setups = [\n",
    "    ModelSetup(\n",
    "        name=\"forward_testing_model\",\n",
    "        use_fixations=True,\n",
    "        **mobilenet_args,\n",
    "        **small_model_args,\n",
    "        **common_args,\n",
    "        **fusion_add_args,\n",
    "    )\n",
    "]\n",
    "\n",
    "for model_setup in all_model_setups:\n",
    "\n",
    "    print_f.print_title(\"Preparing for the training.\")\n",
    "\n",
    "    train_info = TrainingInfo(model_setup)\n",
    "\n",
    "    if model_setup.measure_test:\n",
    "        # initialise the test recording list.\n",
    "        train_info.test_ap_ars = []\n",
    "\n",
    "    model = create_multimodal_rcnn_model(\n",
    "            labels_cols,\n",
    "            model_setup,\n",
    "            rpn_nms_thresh=0.3,\n",
    "            box_detections_per_img=10,\n",
    "            box_nms_thresh=0.2,\n",
    "            rpn_score_thresh=0.0,\n",
    "            box_score_thresh=0.05,\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    ################ Datasets ################\n",
    "    dataset_params_dict = {\n",
    "            \"XAMI_MIMIC_PATH\": XAMI_MIMIC_PATH,\n",
    "            \"dataset_mode\": model_setup.dataset_mode,\n",
    "            \"bbox_to_mask\": model_setup.use_mask,\n",
    "            \"labels_cols\": DEFAULT_REFLACX_LABEL_COLS,\n",
    "    }\n",
    "\n",
    "    detect_eval_dataset, train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "            dataset_params_dict=dataset_params_dict\n",
    "    )\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_dataloaders(\n",
    "            train_dataset, val_dataset, test_dataset, batch_size=2\n",
    "    )\n",
    "\n",
    "    train_coco, val_coco, test_coco = get_cocos(\n",
    "            train_dataloader, val_dataloader, test_dataloader\n",
    "    )\n",
    "\n",
    "#     eval_params_dict = get_eval_params_dict(\n",
    "#             detect_eval_dataset, iou_thrs=iou_thrs, use_iobb=use_iobb,\n",
    "#     )\n",
    "\n",
    "    # dynamic_loss_weight = None\n",
    "    loss_keys = [\n",
    "            \"loss_classifier\",\n",
    "            \"loss_box_reg\",\n",
    "            \"loss_objectness\",\n",
    "            \"loss_rpn_box_reg\",\n",
    "    ]\n",
    "            \n",
    "    dynamic_loss_weight = DynamicWeightedLoss(\n",
    "            keys=loss_keys + [\"loss_mask\"] if model_setup.use_mask else loss_keys\n",
    "    )\n",
    "    dynamic_loss_weight.to(device)\n",
    "    print_params_setup(model)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if dynamic_loss_weight:\n",
    "        params += [p for p in dynamic_loss_weight.parameters() if p.requires_grad]\n",
    "\n",
    "    iou_types = get_iou_types(model, model_setup)\n",
    "    optimizer = get_optimiser(params, model_setup)\n",
    "    lr_scheduler = get_lr_scheduler(optimizer, model_setup)\n",
    "\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    print_f.print_title(\n",
    "        f\"Start training. Preparing Tooke [{ (current_time - train_info.start_t).seconds}] sec\"\n",
    "    )\n",
    "\n",
    "    train_info.start_t = datetime.now()\n",
    "\n",
    "    val_loss = None\n",
    "\n",
    "    ## Start the training from here.\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        print_f.print_title(f\"Training model: [{model_setup.name}]\")\n",
    "        print(train_info)\n",
    "\n",
    "        train_info.epoch = e + 1\n",
    "\n",
    "        if train_info.epoch > model_setup.gt_in_train_till:\n",
    "            model.roi_heads.use_gt_in_train  = False\n",
    "\n",
    "        ###### Perform training and show the training result here ######\n",
    "        model.train()\n",
    "\n",
    "        train_info.last_train_evaluator, train_loger = xami_train_one_epoch(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                data_loader=train_dataloader,\n",
    "                device=device,\n",
    "                epoch=train_info.epoch,\n",
    "                print_freq=10,\n",
    "                iou_types=iou_types,\n",
    "                coco=train_coco,\n",
    "                score_thres=None,\n",
    "                evaluate_on_run=False,\n",
    "                # params_dict=eval_params_dict,\n",
    "                dynamic_loss_weight=dynamic_loss_weight,\n",
    "        )\n",
    "\n",
    "        # train_info.train_evaluators.append(train_evaluator)\n",
    "        train_info.train_losses.append(get_data_from_metric_logger(train_loger))\n",
    "        ################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37134ace218921b74af13dd92c56fdadfb15c5b94d65c3fac1e4c7a849f19ee9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
