{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils.print as print_f\n",
    "\n",
    "from utils.coco_eval import get_eval_params_dict\n",
    "from utils.engine import xami_train_one_epoch, xami_evaluate, get_iou_types\n",
    "from utils.plot import plot_losses, plot_ap_ars, plot_result\n",
    "from utils.save import get_data_from_metric_logger\n",
    "from utils.coco_utils import get_cocos\n",
    "\n",
    "from models.setup import ModelSetup\n",
    "from models.build import create_multimodal_rcnn_model\n",
    "from models.train import TrainingInfo\n",
    "from utils.save import check_best, end_train\n",
    "from data.load import get_datasets, get_dataloaders\n",
    "from IPython.display import clear_output\n",
    "from utils.eval import get_ap_ar, get_ap_ar_for_train_val\n",
    "from utils.train import get_optimiser, get_lr_scheduler, print_params_setup\n",
    "from utils.init import reproducibility, clean_memory_get_device\n",
    "from models.dynamic_loss import DynamicWeightedLoss\n",
    "from data.constants import DEFAULT_REFLACX_LABEL_COLS, XAMI_MIMIC_PATH\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "\n",
    "## Suppress the assignement warning from pandas.r\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "## Supress user warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will running on device: [CPU]\n"
     ]
    }
   ],
   "source": [
    "device = clean_memory_get_device()\n",
    "reproducibility()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_iobb = True\n",
    "io_type_str = \"IoBB\" if use_iobb else \"IoU\"\n",
    "labels_cols = DEFAULT_REFLACX_LABEL_COLS\n",
    "iou_thrs = np.array([0.5])\n",
    "\n",
    "common_args = {\n",
    "    \"save_early_stop_model\": True,\n",
    "    \"optimiser\": \"sgd\",\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"image_backbone_pretrained\": True,\n",
    "    \"heatmap_backbone_pretrained\": True,\n",
    "    \"record_training_performance\": True,\n",
    "    \"image_size\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"warmup_epochs\": 0,\n",
    "    \"lr_scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"reduceLROnPlateau_factor\": 0.1,\n",
    "    \"reduceLROnPlateau_patience\": 999,\n",
    "    \"reduceLROnPlateau_full_stop\": True,\n",
    "    \"multiStepLR_milestones\": 100,\n",
    "    \"multiStepLR_gamma\": 0.1,\n",
    "    \"use_mask\": True,\n",
    "    \"gt_in_train_till\": 999,\n",
    "    \"box_head_dropout_rate\": 0,\n",
    "    \"measure_test\": True,\n",
    "}\n",
    "\n",
    "fusion_add_args = {\n",
    "    \"fuse_depth\": 0,\n",
    "    \"fusion_residule\": False,\n",
    "    \"fusion_strategy\": \"add\", \n",
    "}\n",
    "\n",
    "small_model_args = {\n",
    "    \"mask_hidden_layers\": 64,\n",
    "    \"fuse_conv_channels\": 64,\n",
    "    \"representation_size\": 64, \n",
    "    \"backbone_out_channels\": 64,\n",
    "}\n",
    "\n",
    "mobilenet_args = {\n",
    "    \"backbone\": \"mobilenet_v3\",\n",
    "    \"using_fpn\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_setups = [\n",
    "    ModelSetup(\n",
    "        name=\"CXR_images_pupil\",\n",
    "        use_heatmaps=True,\n",
    "        with_fixations=False,\n",
    "        with_pupil=True,\n",
    "        **mobilenet_args,\n",
    "        **small_model_args,\n",
    "        **common_args,\n",
    "        **fusion_add_args,\n",
    "    ),\n",
    "    ModelSetup(\n",
    "        name=\"CXR_images_fixations\",\n",
    "        use_heatmaps=True,\n",
    "        with_fixations=True,\n",
    "        with_pupil=False,\n",
    "        **mobilenet_args,\n",
    "        **small_model_args,\n",
    "        **common_args,\n",
    "        **fusion_add_args,\n",
    "    ), \n",
    "    ModelSetup(\n",
    "        name=\"CXR_images\",\n",
    "        use_heatmaps=True,\n",
    "        with_fixations=False,\n",
    "        with_pupil=False,\n",
    "        **mobilenet_args,\n",
    "        **small_model_args,\n",
    "        **common_args,\n",
    "        **fusion_add_args,\n",
    "    ), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "train_infos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================Preparing for the training.====================\n",
      "Using pretrained backbone. mobilenet_v3\n",
      "Using pretrained backbone. mobilenet_v3\n",
      "CXR_images_pupil will use mask, [64] layers.\n",
      "Dataloader creating...\n",
      "Dataloader created!!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1092/591550789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mtrain_coco\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     train_coco, val_coco, test_coco = get_cocos(\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\Desktop\\multimodal-abnormalities-detection\\utils\\coco_utils.py\u001b[0m in \u001b[0;36mget_cocos\u001b[1;34m(train_dataloader, val_dataloader, test_dataloader)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_cocos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_coco\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mval_coco\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtest_coco\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\Desktop\\multimodal-abnormalities-detection\\utils\\coco_utils.py\u001b[0m in \u001b[0;36mget_coco_api_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\Desktop\\multimodal-abnormalities-detection\\utils\\coco_utils.py\u001b[0m in \u001b[0;36mconvert_to_coco_api\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;31m# targets = ds.get_annotations(img_idx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;31m# img, clinical_num, clinical_cat, targets = ds[img_idx]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\Desktop\\multimodal-abnormalities-detection\\data\\datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fixations_path\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fixations_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox_to_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mminmax_scale\u001b[1;34m(X, feature_range, axis, copy)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;31m# Unlike the scaler object, this function allows 1d input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;31m# If copy is required, it will be done inside the scaler object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m     X = check_array(X, copy=False, ensure_2d=False,\n\u001b[0m\u001b[0;32m    546\u001b[0m                     dtype=FLOAT_DTYPES, force_all_finite='allow-nan')\n\u001b[0;32m    547\u001b[0m     \u001b[0moriginal_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\VIMMI\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    714\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[0;32m    717\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "## we have to mention that in order to provide objective evaluation, we compromise the dataset size, which also nagatively afftect the performance and generlaization.\n",
    "\n",
    "for model_setup in all_model_setups:\n",
    "\n",
    "    print_f.print_title(\"Preparing for the training.\")\n",
    "\n",
    "    train_info = TrainingInfo(model_setup)\n",
    "\n",
    "    if model_setup.measure_test:\n",
    "        # initialise the test recording list.\n",
    "        train_info.test_ap_ars = []\n",
    "\n",
    "    model = create_multimodal_rcnn_model(\n",
    "        labels_cols,\n",
    "        model_setup,\n",
    "        rpn_nms_thresh=0.3,\n",
    "        box_detections_per_img=10,\n",
    "        box_nms_thresh=0.2,\n",
    "        rpn_score_thresh=0.0,\n",
    "        box_score_thresh=0.05,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    ################ Datasets ################\n",
    "    dataset_params_dict = {\n",
    "        \"XAMI_MIMIC_PATH\": XAMI_MIMIC_PATH,\n",
    "        \"with_fixation\": model_setup.with_fixations,\n",
    "        \"with_pupil\": model_setup.with_pupil,\n",
    "        \"bbox_to_mask\": model_setup.use_mask,\n",
    "        \"labels_cols\": labels_cols,\n",
    "    }\n",
    "\n",
    "    print(\"Dataloader creating...\")\n",
    "\n",
    "    detect_eval_dataset, train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "        dataset_params_dict=dataset_params_dict,\n",
    "    )\n",
    "\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_dataloaders(\n",
    "        train_dataset, val_dataset, test_dataset, batch_size=model_setup.batch_size,\n",
    "    )\n",
    "\n",
    "    print(\"Dataloader created!!\")\n",
    "\n",
    "    train_coco= None\n",
    "\n",
    "    train_coco, val_coco, test_coco = get_cocos(\n",
    "        train_dataloader, val_dataloader, test_dataloader\n",
    "    )\n",
    "\n",
    "    eval_params_dict = get_eval_params_dict(\n",
    "        detect_eval_dataset, iou_thrs=iou_thrs, use_iobb=use_iobb,\n",
    "    )\n",
    "\n",
    "    # dynamic_loss_weight = None\n",
    "    loss_keys = [\n",
    "        \"loss_classifier\",\n",
    "        \"loss_box_reg\",\n",
    "        \"loss_objectness\",\n",
    "        \"loss_rpn_box_reg\",\n",
    "    ]\n",
    "    \n",
    "    dynamic_loss_weight = DynamicWeightedLoss(\n",
    "        keys=loss_keys + [\"loss_mask\"] if model_setup.use_mask else loss_keys\n",
    "    )\n",
    "    dynamic_loss_weight.to(device)\n",
    "    print_params_setup(model)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if dynamic_loss_weight:\n",
    "        params += [p for p in dynamic_loss_weight.parameters() if p.requires_grad]\n",
    "\n",
    "    iou_types = get_iou_types(model, model_setup)\n",
    "    optimizer = get_optimiser(params, model_setup)\n",
    "    lr_scheduler = get_lr_scheduler(optimizer, model_setup)\n",
    "\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    print_f.print_title(\n",
    "        f\"Start training. Preparing Took [{ (current_time - train_info.start_t).seconds}] sec\"\n",
    "    )\n",
    "\n",
    "    train_info.start_t = datetime.now()\n",
    "\n",
    "    val_loss = None\n",
    "\n",
    "    ## Start the training from here.\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        print_f.print_title(f\"Training model: [{model_setup.name}]\")\n",
    "        print(train_info)\n",
    "\n",
    "        train_info.epoch = e + 1\n",
    "\n",
    "        if train_info.epoch > model_setup.gt_in_train_till:\n",
    "            model.roi_heads.use_gt_in_train  = False\n",
    "\n",
    "        ###### Perform training and show the training result here ######\n",
    "        model.train()\n",
    "\n",
    "        train_info.last_train_evaluator, train_loger = xami_train_one_epoch(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_dataloader,\n",
    "            device=device,\n",
    "            epoch=train_info.epoch,\n",
    "            print_freq=10,\n",
    "            iou_types=iou_types,\n",
    "            coco=train_coco,\n",
    "            score_thres=None,\n",
    "            evaluate_on_run=True,\n",
    "            params_dict=eval_params_dict,\n",
    "            dynamic_loss_weight=dynamic_loss_weight,\n",
    "        )\n",
    "\n",
    "        # train_info.train_evaluators.append(train_evaluator)\n",
    "        train_info.train_losses.append(get_data_from_metric_logger(train_loger))\n",
    "        ################################################################\n",
    "\n",
    "        ####### Put the model into evaluation mode, start evaluating the current model #######\n",
    "        model.eval()\n",
    "\n",
    "        train_info.last_val_evaluator, val_logger = xami_evaluate(\n",
    "            model=model,\n",
    "            data_loader=val_dataloader,\n",
    "            device=device,\n",
    "            params_dict=eval_params_dict,\n",
    "            coco=val_coco,\n",
    "            iou_types=iou_types,\n",
    "            score_thres=None,\n",
    "        )\n",
    "\n",
    "        # train_info.val_evaluators.append(val_evaluator)\n",
    "        train_info.val_losses.append(get_data_from_metric_logger(val_logger))\n",
    "\n",
    "        train_ap_ar, val_ap_ar = get_ap_ar_for_train_val(\n",
    "            train_info.last_train_evaluator,\n",
    "            train_info.last_val_evaluator,\n",
    "            areaRng=\"all\",\n",
    "            iouThr=0.5,\n",
    "            maxDets=10,\n",
    "        )\n",
    "\n",
    "        train_info.train_ap_ars.append(train_ap_ar)\n",
    "        train_info.val_ap_ars.append(val_ap_ar)\n",
    "\n",
    "        if model_setup.measure_test:\n",
    "            train_info.test_evaluator, test_logger = xami_evaluate(\n",
    "                model=model,\n",
    "                data_loader=test_dataloader,\n",
    "                device=device,\n",
    "                params_dict=eval_params_dict,\n",
    "                coco=test_coco,\n",
    "                iou_types=iou_types,\n",
    "                score_thres=None,\n",
    "            )\n",
    "            train_info.test_losses.append(get_data_from_metric_logger(test_logger))\n",
    "            test_ap_ar = get_ap_ar(\n",
    "                train_info.test_evaluator, areaRng=\"all\", iouThr=0.5, maxDets=10,\n",
    "            )\n",
    "            train_info.test_ap_ars.append(test_ap_ar)\n",
    "\n",
    "        ### update the learning rate\n",
    "\n",
    "        val_loss = train_info.val_losses[-1][\"loss\"]\n",
    "\n",
    "        if train_info.epoch > model_setup.warmup_epochs:\n",
    "            if not lr_scheduler is None:\n",
    "                if isinstance(lr_scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    if (\n",
    "                        model_setup.reduceLROnPlateau_full_stop\n",
    "                        and lr_scheduler.num_bad_epochs\n",
    "                        >= model_setup.reduceLROnPlateau_patience\n",
    "                    ):\n",
    "                        print_f.print_title(\n",
    "                            f\"| EarlyStop | Epoch [{train_info.epoch}] Done | It has took [{sec_took/60:.2f}] min, Avg time: [{speed:.2f}] sec/epoch | Estimate time for [{num_epochs}] epochs: [{speed*num_epochs/60:.2f}] min | Epoch took [{epoch_took.seconds}] sec |\"\n",
    "                        )\n",
    "                        break\n",
    "                    lr_scheduler.step(val_loss)\n",
    "                else:\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "        ## Clean everything before we show the evaluating result in this stage, so we can inspect the training progress.\n",
    "        clear_output()\n",
    "\n",
    "        # if model_setup.record_training_performance:\n",
    "        plot_ap_ars(\n",
    "            train_ap_ars=train_info.train_ap_ars,\n",
    "            val_ap_ars=train_info.val_ap_ars,\n",
    "            test_ap_ars=train_info.test_ap_ars,\n",
    "        )\n",
    "\n",
    "        plot_losses(train_info.train_losses, train_info.val_losses, test_logers=train_info.test_losses)\n",
    "\n",
    "        previous_time = current_time\n",
    "        current_time = datetime.now()\n",
    "        epoch_took = current_time - previous_time\n",
    "\n",
    "        sec_took = (current_time - train_info.start_t).seconds\n",
    "        speed = sec_took / train_info.epoch\n",
    "\n",
    "        print_str = f\"| Epoch [{train_info.epoch}] Done | It has took [{sec_took/60:.2f}] min, Avg time: [{speed:.2f}] sec/epoch | Estimate time for [{num_epochs}] epochs: [{speed*num_epochs/60:.2f}] min | Epoch took [{epoch_took.seconds}] sec | \"\n",
    "\n",
    "        if lr_scheduler and hasattr(lr_scheduler, \"num_bad_epochs\"):\n",
    "            print_str += f\"Patience [{lr_scheduler.num_bad_epochs}] |\"\n",
    "\n",
    "        print_f.print_title(print_str)\n",
    "\n",
    "        #######################################################################################\n",
    "        if model_setup.save_early_stop_model:\n",
    "            val_ar, val_ap, train_info = check_best(\n",
    "                val_ap_ar=val_ap_ar,\n",
    "                device=device,\n",
    "                eval_params_dict=eval_params_dict,\n",
    "                train_info=train_info,\n",
    "                model=model,\n",
    "                optim=optimizer,\n",
    "                test_dataloader=test_dataloader,\n",
    "                test_coco=test_coco,\n",
    "                iou_types=iou_types,\n",
    "                score_thres=None,\n",
    "                dynamic_weight=dynamic_loss_weight,\n",
    "            )\n",
    "\n",
    "    val_ap_ar = get_ap_ar(train_info.last_val_evaluator)\n",
    "\n",
    "    train_info = end_train(\n",
    "        train_info=train_info,\n",
    "        model=model,\n",
    "        optim=optimizer,\n",
    "        eval_params_dict=eval_params_dict,\n",
    "        last_val_ar=val_ap_ar[\"ar\"],\n",
    "        last_val_ap=val_ap_ar[\"ap\"],\n",
    "        test_dataloader=test_dataloader,\n",
    "        device=device,\n",
    "        test_coco=test_coco,\n",
    "        iou_types=iou_types,\n",
    "        score_thres=None,\n",
    "        dynamic_weight=dynamic_loss_weight,\n",
    "    )\n",
    "\n",
    "    train_infos.append(train_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_info in train_infos:\n",
    "#     print(train_info)\n",
    "#     model_setup = train_info.model_setup\n",
    "#     model = create_multimodal_rcnn_model(\n",
    "#         labels_cols,\n",
    "#         model_setup,\n",
    "#         rpn_nms_thresh=0.3,\n",
    "#         box_detections_per_img=10,\n",
    "#         box_nms_thresh=0.2,\n",
    "#         rpn_score_thresh=0.0,\n",
    "#         box_score_thresh=0.05,\n",
    "#     )\n",
    "#     print_params_setup(model)\n",
    "#     print(f\"Max AP on test: [{max([ap_ar['ap']  for ap_ar in  train_info.test_ap_ars]):.4f}]\")\n",
    "#     plot_ap_ars(\n",
    "#             train_ap_ars=train_info.train_ap_ars,\n",
    "#             val_ap_ars=train_info.val_ap_ars,\n",
    "#             test_ap_ars=train_info.test_ap_ars,\n",
    "#         )\n",
    "\n",
    "#     plot_losses(train_info.train_losses, train_info.val_losses, test_logers=train_info.test_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2b4c68e18acfab416f9980befd91f6d572217495ef701c2da267a02b76310ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
